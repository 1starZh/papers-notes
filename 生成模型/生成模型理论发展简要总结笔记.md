
## 什么是生成模型？

生成模型，顾名思义，从这个模型中可以生成新的数据，而我们研究生成模型的目的就是为了得到这样一个模型，从而使其可以产生我们所需要的新数据。为了得到这样的模型，我们需要更深入的探究这个**模型**的实质，我们假设已有的数据服从一个概率分布$p_{data}(x)$，我们获得新数据即是从这个分布当中取新的样本。在认识上面所述事实的条件下 ，我们的目的转变为了得到这样的一个分布$p_{data}(x)$，但是，直接获取这样的分布往往是一个很困难的事情，所以我们需要用一个分布$p_{model}(x)$去逼近这样的一个分布。我们研究生成模型就是为了获得这样的一个$p_{model}(x)$。

## 经典概率模型（简单了解）
### 高斯混合

#### 混合模型
混合模型是一个可以用来表示在总体分布（distribution）中含有 K 个子分布的概率模型，换句话说，混合模型表示了观测数据在总体中的概率分布，它是一个由 K 个子分布组成的混合分布。混合模型不要求观测数据提供关于子分布的信息，来计算观测数据在总体分布中的概率。
#### 高斯模型
##### 单高斯模型
当样本数据$X$是一维数据时，高斯分布遵循下面的概率密度函数
$$
P(x|\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$
其中$\mu$是均值，$\sigma$是标准差

当样本数据$X$是多维数据时，高斯分布遵循下面的概率密度函数
$$
P(x|\theta) = \frac{1}{(2\pi)^{\frac{D}{2}} |\boldsymbol{\Sigma}|^{\frac{1}{2}}} 
\exp\left(
    -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\top 
    \boldsymbol{\Sigma}^{-1} 
    (\mathbf{x} - \boldsymbol{\mu})
\right)
$$
其中$\Sigma$是协方差，$D$是数据维度

##### 混合模型

高斯混合模型可以看作是由K个单高斯模型组合而成的，这K个子模型可以看作是高斯混合模型的隐变量。

高斯混合模型的概率分布：
$$
P(x|\theta)=\Sigma_{k=1}^K\alpha_k\phi(x|\theta_k)
$$
其中：
$x_j$是观测数据
$K$是混合模型中子模型的数量
$\alpha_k$是观测数据属于第$k$个子模型的概率
$\phi(x|\theta_k)$是第$k$个子模型的概率密度函数

对于这个模型而言，参数就是每个子模型的期望、方差（或协方差）、在混合模型中发生的概率。

对于混合高斯模型可以使用$EM$算法求解其最大似然估计，见李航老师的机器学习方法，不多叙述。

#### 隐式马尔科夫模型

##### $n$阶马尔科夫过程
状态间的转移仅依赖前$n$个状态的过程就叫做$n$阶马尔科夫过程
$$
p(q_t=S_j|q_{t-1}=S_{i}, q_{t-2}=S_{i-1}, \cdots, q_{t-n}=S_{i-n+1})
$$
##### 隐式马尔科夫模型（HMM）

一个隐式马尔科夫模型一般记为$\lambda=[\pi, A, B]$,下面我们来解释一下这些符号的意义。

$\pi$是模型在初始状态下出现各状态的概率$\pi=(\pi_1, \cdots, \pi_n)$,$\pi_i$是初始状态是$S_i$的概率
$A$是各个状态直接转换的概率，通常记作矩阵$A[a_{ij}]$
$B$式根据当前状态获得各个观测值的概率，记作矩阵$B[b_{ij}]$

相对于马尔可夫模型，隐马尔可夫只是多了一个各状态的观测概率。
该模型是一个**双重随机过程**，包括**模型的状态转换**和**特定状态下可观察事件的随机**。


以上我们简单叙述了两种比较重要的机器学习生成预测模型，虽然这种模型本身已经被基于深度学习的很多方法超越，但是我认为在深度学习时代下，学习机器学习模型依旧是非常必要的。传统机器学习方法会教给我们很多思考方式，这种思考方式在很多时候可以帮助我们解决遇到的各种问题。

## VAE

### 变分贝叶斯

文章的摘要中提到了变分贝叶斯方法，我们在这里简单介绍一下

在贝叶斯的世界里，我们需要求解的是参数的后验分布
$$
p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)}
$$
对于问题
$$
p(x)=\int_{}p(x|\theta)p(\theta)d\theta
$$
这个积分往往是不可以积分出来的，所以这个时候我们就需要**变分**，我们找一个比较好算的分布$q(x)$来近似$p(\theta|x)$，为了衡量这两个分布之间的距离，我们是使用KL散度
$$
KL(q(\theta)||p(\theta|x))=\int q(\theta)log\frac{q(\theta)}{p(\theta|x)}d\theta
$$
变换得到ELBO

$$
KL(q(\theta)||p(\theta|x))=log\ p(x)-ELBO(q)
$$
最小化KL散度可以被转变为最大化ELBO，这就是变分贝叶斯的核心逻辑，**将不可解的积分问题转变为优化问题**

### 马尔科夫蒙特卡洛方法

后面再补上这一部分讲解，思想比较简单，是用采样去解复杂积分。

### VAE

VAE这篇文章做出的主要贡献是引入了一种新的变分下界估计器，SGVB，用于连续潜变量的有效近似推理。

如我们在变分贝叶斯一节所提到的，我们可以通过优化ELBO来解决对KL散度的优化问题。
KL散度可以写作如下的形式
$$
\mathcal{L}(\theta, \phi; x)=-D_{KL}(q_{\phi}(z|x)||p_{\theta}|(z))+\mathbb{E}_{q_{\phi}(z|x)}[log\ p_{\theta}(x|z)]
$$
我们想要优化这个变分下界，但是普通的蒙特卡洛梯度估计会存在由非常高的方差，这是不work的。

本文提出的SGVB方法为我们提供了解决这一优化问题的方法。
我们对近似后验$q_{\phi}(z|x)$重新参数化，我们令随机变量$\tilde{z}\sim g_{\phi}(\varepsilon,x)$ $with$ $\varepsilon\sim p(\varepsilon)$,我们会得到如下式子：

![](pic/final_result.png)
之后就可以使用随机梯度下降等方法进行优化了。

算法流程如下
![](pic/suanfa_VAE.png)
下面是文章中举的一个使用全连接神经网络（MLP）来做近似后验的例子。

![VAE](pic/VAE.png)

该例子当中，使用了隐藏层数相同的MLP作为该方法的decoder和encoder，令隐变量服从标准正态分布。

### $\beta-VAE$

上文我们介绍了VAE模型，ELBO可以写为

$$
\mathcal{L}(\theta, \phi; x)=-D_{KL}(q_{\phi}(z|x)||p_{\theta}|(z))+\mathbb{E}_{q_{\phi}(z|x)}[log\ p_{\theta}(x|z)]
$$
其中，第一项可被认为是正则约束项，这部分约束近似后验逼近真实分布，第二项可以被认为是重建损失。
改进的$\beta-VAE$引入了一个超参数$\beta$,现在的ELBO可以被写作
$$
\mathcal{L}(\theta, \phi; x)=-\beta D_{KL}(q_{\phi}(z|x)||p_{\theta}|(z))+\mathbb{E}_{q_{\phi}(z|x)}[log\ p_{\theta}(x|z)]
$$
当 $\beta > 1$时，KL 项被放大，模型更强烈地约束潜空间与先验分布一致。

为了解释这种改变带来的效果，我们对ELBO继续进行变换
![](pic/beta-VAE.png)

KL可以分解为这三项的和，放大这三项的和就加强了模型的解耦能力，但是这其实不是准确的做法，我们应该将这三项分开处理，直接决定解耦的是TC，他是测量维度间的依赖的项，这也是后续工作的做法。

### VQ-VAE

在传统的VAE中，将潜变量建模为连续空间，一般是高斯，这种处理会有所谓后验崩溃的问题，在本文中，作者提出了将潜变量空间建模为一个离散表示的方法，具体到应用就是构建一个$embedding\ space$， 用这个空间中的离散向量来表示特征。

这篇文章最重要的就是他提出的codebook方法，启发了很多之后的工作，所以我们接下来介绍一下这个方法。
![](pic/VQ-VAE.png)
这个方法构建了一个K×D维的向量空间，通过编码器产生$z_e(x)$,然后通过最近邻查找表中与其最相近的向量$z_q(x)$，使用这个向量作为解码器的输入。

其比较重要的部分还有对其损失项梯度传递的处理，简要说一下：

![](pic/vq-vae-loss.png)
由于过程中存在不可导的操作，如从$z_e(x)$到$z_q(x)$的过程，所以梯度不能直接进行回传，需要特殊处理，作者提出了一种前向与后向传播不对应的算法：
$$
sg(x)=\left\{
\begin{aligned}
x & \ (forward)\\ 
0 & \ (backward)
\end{aligned}
\right.
$$
经过该操作处理，可以将损失写为
![](pic/vqlosschange.png)
## GAN

接下来我们看一下GAN网络，这是一个不同于VAE的生成模型范式，其目的还是为了解决极大似然估计难以处理的属性，所以提出了使用辨别器辅助来实现对真实分布的近似。

为了得到与真实分布$p_{data}(x)$近似的简单分布$p_{\theta}(x)$我们构造生成网络G，生成网络G的目的是为了将随机噪声$z$输入G网络得到输出$x$，G的参数由神经网络决定。为了优化G的参数使其能够更加靠近真实分布，我们构建一个辨别器D，D的作用是将$x$输入进去从而得出$x$是来自真实分布的概率（0，1）。所以GAN的最终目标就是得到一个$G^*$使得其满足
$$
G^*=arg\ min_G\ max_D\ V(G,D)
$$
关于V
$$
V(G, D)=E_{X\sim P_{data}}[log\ D(x)]+E_{X\sim P_{g}}[log\ (1-D(x))]
$$
![](pic/GAN.png)
这张图很好了讲解了GAN的训练过程，其最后D会收敛到1/2的位置 ，而生成其则可以近似到真实的数据分布上面去。

其收敛唯一性可被证明，最后$p_g$会收敛到$p_{data}$。

GAN的训练过程比较困难，生成器G容易找到可以骗过D的捷径导致生成不具备多样性，如果G与D的实力不均衡，会导致另一方无法学习，GAN的训练会产生震荡。


为了解决GAN难以训练的问题，后续工作做出了一些改进，我们简单介绍几个

### DCGAN

- 用卷积与反卷积（ConvTranspose）替代原始 GAN 的全连接层，让网络天然适合图像的空间结构（平移不变、局部相关）。
    
- 生成器逐步把低维噪声（向量）上采样成图像（use ConvTranspose2d）；判别器使用常规卷积下采样判真假（use Conv2d）。
    
- 关键规范化：在大多数隐藏层用 BatchNorm，有助于训练稳定（Generator 和 Discriminator 除了输入/输出层外）。
    
- 激活函数：Generator 用 ReLU（输出层 Tanh）；Discriminator 用 LeakyReLU（输出层 Sigmoid）。
    
- 权重初始化：正态分布（mean=0, std=0.02）是 DCGAN 的经验法则。

设计原则如下

- **去掉池化（pooling）层**，使用 stride>1 的卷积/反卷积来代替下采样/上采样（网络自己学过滤和抽样）。
    
- **在 Generator 中隐藏层使用 ReLU，最后一层用 tanh**（输出 [-1,1]）。
    
- **在 Discriminator 中使用 LeakyReLU，最后一层用 sigmoid**（输出概率）。
    
- **在 Generator 和 Discriminator 的绝大多数层使用 BatchNorm**（有助于优化）。
    
- **不要在 Generator 或 Discriminator 的输入或输出直接使用 BatchNorm**（因为输入/输出归一化问题）。
    
- **权重初始化为 Normal(0, 0.02)**（每次实验都做的事，帮助模型稳定开始训练）。

### Wasserstein GAN


这篇文章对GAN网络之所以会出现难以训练的问题做了系统性的分析，主要有以下两部分

- 1.第一点是JS散度的问题，GAN的思想是要将分布$p_g$拉向$p_{data}$，于是引入和JS散度作为衡量两个分布之间距离的标准， 但是当这两个分布之间没有重叠的部分或者重叠的部分可以忽略（不可解）的时候JS散度将恒为常数$log\ 2$,这也就意味着梯度将恒为0，那么此时生成器G将学习不到有用的信息。那么这两个分布出现不重叠或者重叠部分可以忽略的几率大吗，答案是：是的，当两个分布是高维空间中的低维流形时，他们的重叠部分测度为0的概率是1，而这两个分布经过编码被映射到一个非常高的维度（比如4096的特征）时，也许真正有用的特征维度很低（比如只有100维的信息是有用的）那么此时这两个分布就会使高维空间的低维流形，他们之间有非常大的概率没有重叠部分，导致GAN非常难以训练。这里本质时阐述了用JS散度来衡量这两个分布之间的距离的不合理性，我们应该要选择一个更加合理的衡量标准。
- 2.第二点是对其损失另一角度的分析，其生成器损失项$E_{x\sim P_g}[-log\ D(x)]$可以被变形为$KL(p_g||p_r)-2JS(p_g||p_r)$，我们发现这个损失一边要将两个分布拉近一边要将两个分布推远，这是矛盾的，所以其损失会非常的抖动。这里的KL散度项也是有问题的，这个KL散度对生成器没能生成正确的样本的惩罚较小，而对生成器生成了不真实的样本惩罚很大，导致生成器很可能不愿意生成多样性的样本。

为了解决上述的问题，作者引入了Wasserstein距离（又叫Earth-Mover（EM）距离）
$$
W(P_r,P_g)=inf_{\gamma\sim \Pi (P_r,P_g)}\ \mathbb{E}_{(x,y)\sim \gamma}[||x-y||]
$$
用这种方法衡量两个分布的距离，可以提供稳定的梯度，不会产生上述两种问题中的梯度消失及突变问题。

WGAN相对于原始的GAN其实只改动了以下4点

- 判别器最后一层去掉sigmoid
- 生成器和判别器的loss不取log
- 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
- 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

其中第四点是实验结论。
算法流程如下：

![](pic/WGAN.PNG)
### styleGAN

![](pic/styleGAN.png)

styleGAN提出了一种基于风格迁移的生成器。接下里介绍一下该文章提出的创新之处：

- 抛弃了原始GAN直接将潜变量直接作为生成器的输入，而是将一个可学习的常数作为输入，这一步可以减少网络的特征耦合。
- 使用映射网络训练出一个不跟随数据分布的参数$W$,可以减少特征之间的相关性，即解耦合。
- W通过每个卷积层的AdaIN输入到生成器的每一层中。图中的A代表一个可学习的仿射变换。
- 使用$AdaIN(x_i,y)=y_{s,i}\frac{x_i-\mu(x_i)}{\sigma(x_i)}+y_{b,i}$将W添加到生成网络的每一个卷积部分后面，其中A是一个可学习的仿射变换。（这一步会导致水滴问题，应去掉AdaIN操作）
- 通过引入噪声为生成器生成随机细节

### FLOW

FLOW的最重要贡献是使用变量替换方法，用构造雅可比矩阵的方法精确似然估计，我们在之前的VAE提到过，VAE使用的方法是最大化似然函数的ELBO，而不是最大化似然函数本身，FLOW的提出成功解决了似然函数无法精确的这一问题。

简单介绍其思想：
存在可逆函数f，$z=f(x)$,使用变量替换就可以得到
![](pic/FLOW.png)
构造了一系列的可逆变换，得到了可优化的最大似然，由于f是可求逆的，所以我们计算f的逆就得到了生成模型。

### NCIE

上述目标函数虽然可以优化但是雅各比矩阵行列式值的计算是困难的，这篇文章提出了一种方法，使用耦合层将雅各比矩阵变换为三角矩阵，这样他的行列式值就非常好计算了。

将x的前d维表示为x1，后面表示为x2，采用变换
$$
h_1=x1\
h_2=x_2+m(x_1)
$$
$$J=
\begin{bmatrix}
I_d,\ 0\\
\frac{\partial h_2}{\partial x_1},\ \frac{\partial h_2}{\partial x_2}
\end{bmatrix}
$$
其值为1.
我们使用交替变换的方法去增加非线性性。最后我们会引入一个缩放矩阵S，所以最后行列式的值就为缩矩阵对角线元素之和。

最后我们的优化目标可以变形为：

$$
log\ P_{x_0}(x_0)=\Sigma_{i=1}^D
(log\ s_i-\frac{h_i^2}{2})
$$
注：每个维度的$s_i$越大说明这个维度信息越不重要，因为如果，$h_i$会与其相反，阻止其增大。

### diffusion model

受到非平衡热力学的启发（我觉得也有FLOW的影响），本文通过构造一个加噪声的马尔科夫链一步步对输入图像添加噪声，经过$T$步后原始输入$x_0$变为$x_T$，其最后变为的$x_T$满足正态分布。有了加噪声将图像变为噪声的过程，我们要生成模型只需推导加噪声的逆过程，也就是去噪声的过程即可完成图像的生成。

这里有一篇博客对diffusion model的讲解非常好，深入浅出，详细可以参考
[diffusion model]([扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现 | 周弈帆的博客](https://zhouyifan.net/2023/07/07/20230330-diffusion-model/))

我们在这里简要的介绍一下论文中核心部分思想（不涉及详细的推导）
![](pic/diffusion.png)
论文中的图二很好的描述了扩散模型的基本过程，其前向过程是一步步向图片中注入随机噪声，逆向过程是去噪过程，最后得到清晰的图片。
扩散模型依旧是需要求一个极大似然估计$p_{\theta}(x_0):=\int p_{\theta}(x_{0:T})d_{x_{1:T}}$.其中$x_0\sim q(x_0)$.
![](pic/diff-eq1.png)
公式1中给出了扩散逆向过程的表达式，其中$p(x_T)$是一个标准正态分布，每一个去噪过程也是一个正态分布。

![](pic/diff-eq2.png)
公式2描述了加噪的前向过程，每一步的加噪是一个正态分布，由于为了让每一次加噪对图片产生的影响相近所以我们需要引入一个参数$\beta$控制每一次加噪的程度，这里解释一下什么叫做对图片产生的影响，在初始图片上我们对图片注入细小噪声就可以对图片产生比较的分布上的影响，而在经过很多步后，图片已经注入了很多噪声，再对其注意和开始一样的噪声那么对图片本身产生的影响就没有开始那么大，为了保持一致性，所以在之后我们注入噪声的程度会增加，在公式中既是$\beta_t$随着时间的增长而增长。

通过数学上的变换，我们可以得到对t时刻的噪声注入可以一次完成
![](/pic/diff-eq3.png)
$\alpha_t=1-\beta_t$,$\bar \alpha_t = \Pi_{s=1}^t \alpha_s$.

我们的优化目标如下：
![](pic/diff-eq5.png)
优化这个目标通过数学推导可以变换优化神经网络拟合的生成$x_t$时的随机噪声与真实随机噪声之间的交叉熵损失。

综上，算法流程如下：
![](pic/diff-suanfa.png)

### 总结
以上，我们大体介绍了生成模型发展历史上一些比较重要的文章的思想。
