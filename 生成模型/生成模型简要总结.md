
## 什么是生成模型？

生成模型，顾名思义，从这个模型中可以生成新的数据，而我们研究生成模型的目的就是为了得到这样一个模型，从而使其可以产生我们所需要的新数据。为了得到这样的模型，我们需要更深入的探究这个**模型**的实质，我们假设已有的数据服从一个概率分布$p_{data}(x)$，我们获得新数据即是从这个分布当中取新的样本。在认识上面所述事实的条件下 ，我们的目的转变为了得到这样的一个分布$p_{data}(x)$，但是，直接获取这样的分布往往是一个很困难的事情，所以我们需要用一个分布$p_{model}(x)$去逼近这样的一个分布。我们研究生成模型就是为了获得这样的一个$p_{model}(x)$。

## 经典概率模型（简单了解）
### 高斯混合

#### 混合模型
混合模型是一个可以用来表示在总体分布（distribution）中含有 K 个子分布的概率模型，换句话说，混合模型表示了观测数据在总体中的概率分布，它是一个由 K 个子分布组成的混合分布。混合模型不要求观测数据提供关于子分布的信息，来计算观测数据在总体分布中的概率。
#### 高斯模型
##### 单高斯模型
当样本数据$X$是一维数据时，高斯分布遵循下面的概率密度函数
$$
P(x|\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$
其中$\mu$是均值，$\sigma$是标准差

当样本数据$X$是多维数据时，高斯分布遵循下面的概率密度函数
$$
P(x|\theta) = \frac{1}{(2\pi)^{\frac{D}{2}} |\boldsymbol{\Sigma}|^{\frac{1}{2}}} 
\exp\left(
    -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\top 
    \boldsymbol{\Sigma}^{-1} 
    (\mathbf{x} - \boldsymbol{\mu})
\right)
$$
其中$\Sigma$是协方差，$D$是数据维度

##### 混合模型

高斯混合模型可以看作是由K个单高斯模型组合而成的，这K个子模型可以看作是高斯混合模型的隐变量。

高斯混合模型的概率分布：
$$
P(x|\theta)=\Sigma_{k=1}^K\alpha_k\phi(x|\theta_k)
$$
其中：
$x_j$是观测数据
$K$是混合模型中子模型的数量
$\alpha_k$是观测数据属于第$k$个子模型的概率
$\phi(x|\theta_k)$是第$k$个子模型的概率密度函数

对于这个模型而言，参数就是每个子模型的期望、方差（或协方差）、在混合模型中发生的概率。

对于混合高斯模型可以使用$EM$算法求解其最大似然估计，见李航老师的机器学习方法，不多叙述。

#### 隐式马尔科夫模型

##### $n$阶马尔科夫过程
状态间的转移仅依赖前$n$个状态的过程就叫做$n$阶马尔科夫过程
$$
p(q_t=S_j|q_{t-1}=S_{i}, q_{t-2}=S_{i-1}, \cdots, q_{t-n}=S_{i-n+1})
$$
##### 隐式马尔科夫模型（HMM）

一个隐式马尔科夫模型一般记为$\lambda=[\pi, A, B]$,下面我们来解释一下这些符号的意义。

$\pi$是模型在初始状态下出现各状态的概率$\pi=(\pi_1, \cdots, \pi_n)$,$\pi_i$是初始状态是$S_i$的概率
$A$是各个状态直接转换的概率，通常记作矩阵$A[a_{ij}]$
$B$式根据当前状态获得各个观测值的概率，记作矩阵$B[b_{ij}]$

相对于马尔可夫模型，隐马尔可夫只是多了一个各状态的观测概率。
该模型是一个**双重随机过程**，包括**模型的状态转换**和**特定状态下可观察事件的随机**。


以上我们简单叙述了两种比较重要的机器学习生成预测模型，虽然这种模型本身已经被基于深度学习的很多方法超越，但是我认为在深度学习时代下，学习机器学习模型依旧是非常必要的。传统机器学习方法会教给我们很多思考方式，这种思考方式在很多时候可以帮助我们解决遇到的各种问题。

## VAE

### 变分贝叶斯

文章的摘要中提到了变分贝叶斯方法，我们在这里简单介绍一下

在贝叶斯的世界里，我们需要求解的是参数的后验分布
$$
p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)}
$$
对于问题
$$
p(x)=\int_{}p(x|\theta)p(\theta)d\theta
$$
这个积分往往是不可以积分出来的，所以这个时候我们就需要**变分**，我们找一个比较好算的分布$q(x)$来近似$p(\theta|x)$，为了衡量这两个分布之间的距离，我们是使用KL散度
$$
KL(q(\theta)||p(\theta|x))=\int q(\theta)log\frac{q(\theta)}{p(\theta|x)}d\theta
$$
变换得到ELBO

$$
KL(q(\theta)||p(\theta|x))=log\ p(x)-ELBO(q)
$$
最小化KL散度可以被转变为最大化ELBO，这就是变分贝叶斯的核心逻辑，**将不可解的积分问题转变为优化问题**

### 马尔科夫蒙特卡洛方法

后面再补上这一部分讲解，思想比较简单，是用采样去解复杂积分。

### VAE

VAE这篇文章做出的主要贡献是引入了一种新的变分下界估计器，SGVB，用于连续潜变量的有效近似推理。

如我们在变分贝叶斯一节所提到的，我们可以通过优化ELBO来解决对KL散度的优化问题。
KL散度可以写作如下的形式
$$
\mathcal{L}(\theta, \phi; x)=-D_{KL}(q_{\phi}(z|x)||p_{\theta}|(z))+\mathbb{E}_{q_{\phi}(z|x)}[log\ p_{\theta}(x|z)]
$$
我们想要优化这个变分下界，但是普通的蒙特卡洛梯度估计会存在由非常高的方差，这是不work的。

本文提出的SGVB方法为我们提供了解决这一优化问题的方法。
我们对近似后验$q_{\phi}(z|x)$重新参数化，我们令随机变量$\tilde{z}\sim g_{\phi}(\varepsilon,x)$ $with$ $\varepsilon\sim p(\varepsilon)$,我们会得到如下式子：

![](pic/final_result.png)
之后就可以使用随机梯度下降等方法进行优化了。

算法流程如下
![](pic/suanfa_VAE.png)
下面是文章中举的一个使用全连接神经网络（MLP）来做近似后验的例子。

![VAE](pic/VAE.png)

该例子当中，使用了隐藏层数相同的MLP作为该方法的decoder和encoder，令隐变量服从标准正态分布。

### $\beta-VAE$

上文我们介绍了VAE模型，ELBO可以写为

$$
\mathcal{L}(\theta, \phi; x)=-D_{KL}(q_{\phi}(z|x)||p_{\theta}|(z))+\mathbb{E}_{q_{\phi}(z|x)}[log\ p_{\theta}(x|z)]
$$
其中，第一项可被认为是正则约束项，这部分约束近似后验逼近真实分布，第二项可以被认为是重建损失。
改进的$\beta-VAE$引入了一个超参数$\beta$,现在的ELBO可以被写作
$$
\mathcal{L}(\theta, \phi; x)=-\beta D_{KL}(q_{\phi}(z|x)||p_{\theta}|(z))+\mathbb{E}_{q_{\phi}(z|x)}[log\ p_{\theta}(x|z)]
$$
当 $\beta > 1$时，KL 项被放大，模型更强烈地约束潜空间与先验分布一致。

为了解释这种改变带来的效果，我们对ELBO继续进行变换
![](pic/beta-VAE.png)

KL可以分解为这三项的和，放大这三项的和就加强了模型的解耦能力，但是这其实不是准确的做法，我们应该将这三项分开处理，直接决定解耦的是TC，他是测量维度间的依赖的项，这也是后续工作的做法。





